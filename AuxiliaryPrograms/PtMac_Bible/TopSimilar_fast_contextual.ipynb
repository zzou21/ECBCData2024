{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding Top Similar Words**\n",
    "\n",
    "Author: Lucas Ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ./fine-tuned-MacBERTh and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words most similar to 'divine':\n",
      "0.44577479362487793: #\n",
      "0.4430214464664459: tooth\n",
      "0.4426579773426056: dead\n",
      "0.44222792983055115: death\n",
      "0.4397338628768921: work\n",
      "0.4395444691181183: grace\n",
      "0.4383096694946289: equity\n",
      "0.4339000880718231: write\n",
      "0.4326639175415039: galath\n",
      "0.4303591549396515: purpose\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import heapq\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = './fine-tuned-MacBERTh'  # Path to your fine-tuned model\n",
    "# model_name = \"emanjavacas/MacBERTh\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to get contextual embeddings for a chunk of text\n",
    "def get_contextual_embeddings(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state, inputs\n",
    "\n",
    "# Function to split text into chunks of a given size\n",
    "def split_text_into_chunks(text, tokenizer, chunk_size=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = [' '.join(tokens[i:i + chunk_size]) for i in range(0, len(tokens), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Function to get the embedding for a specific word\n",
    "def get_word_embedding(word, tokenizer, model):\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\", padding='max_length', max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    return embeddings.squeeze()\n",
    "\n",
    "def find_top_similar_words(target_word, text, tokenizer, model, top_n=10):\n",
    "    # Split the text into chunks\n",
    "    chunks = split_text_into_chunks(text, tokenizer)\n",
    "\n",
    "    # Tokenize and get embedding for the target word\n",
    "    target_embedding = get_word_embedding(target_word, tokenizer, model)\n",
    "\n",
    "    # Collect similarities across all chunks\n",
    "    similarities = []\n",
    "    for chunk in chunks:\n",
    "        contextual_embeddings, inputs = get_contextual_embeddings(chunk, tokenizer, model)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        \n",
    "        # Find embeddings for each word in context\n",
    "        for i, (token, embedding) in enumerate(zip(tokens, contextual_embeddings[0])):\n",
    "            if token in tokenizer.all_special_tokens:\n",
    "                continue\n",
    "            similarity = cosine_similarity(target_embedding.unsqueeze(0), embedding.unsqueeze(0)).item()\n",
    "            similarities.append((token, similarity))\n",
    "    \n",
    "    # Use a heap to find the top N similar words\n",
    "    top_entries = []\n",
    "    seen_words = set()\n",
    "    for token, similarity in similarities:\n",
    "        if token not in seen_words:\n",
    "            heapq.heappush(top_entries, (similarity, token))\n",
    "            seen_words.add(token)\n",
    "\n",
    "            if len(top_entries) > top_n:\n",
    "                removed_similarity, removed_token = heapq.heappop(top_entries)\n",
    "                seen_words.remove(removed_token)\n",
    "\n",
    "    top_entries.sort(reverse=True, key=lambda x: x[0])\n",
    "    return top_entries\n",
    "\n",
    "# Function to read text from a file\n",
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Path to the .txt file\n",
    "file_path = 'data/bible_full_text.txt'\n",
    "\n",
    "# Read the text from the file\n",
    "text = read_text_from_file(file_path)\n",
    "\n",
    "# Target word to find similarities with\n",
    "target_word = \"god\"\n",
    "\n",
    "# Find and print top 10 similar words\n",
    "top_similar_words = find_top_similar_words(target_word, text, tokenizer, model, top_n=10)\n",
    "print(f\"Top 10 words most similar to '{target_word}':\")\n",
    "for similarity, word in top_similar_words:\n",
    "    print(f\"{similarity}: {word}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
