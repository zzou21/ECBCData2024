{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding Top Similar Words**\n",
    "\n",
    "Author: Lucas Ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ./fine-tuned-MacBERTh and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words most similar to 'natiue':\n",
      "god: 0.9105949997901917\n",
      "angry: 0.9092453718185425\n",
      "world: 0.907972514629364\n",
      "touch: 0.9073593616485596\n",
      "john: 0.9065189361572266\n",
      "wrought: 0.9064560532569885\n",
      "none: 0.9043810367584229\n",
      "find: 0.9040130376815796\n",
      "argue: 0.9038161635398865\n",
      "preach: 0.90345299243927\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = './fine-tuned-MacBERTh'  # Path to your fine-tuned model\n",
    "# model_name = \"emanjavacas/MacBERTh\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to get word embedding in context\n",
    "def get_contextual_embeddings(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state, inputs\n",
    "\n",
    "def find_top_similar_words(target_word, text, tokenizer, model, top_n=10):\n",
    "    # Get contextual embeddings for the entire text\n",
    "    contextual_embeddings, inputs = get_contextual_embeddings(text, tokenizer, model)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Tokenize and get embedding for the target word\n",
    "    target_embedding = get_word_embedding(target_word, tokenizer, model)\n",
    "    \n",
    "    # Find embeddings for each word in context\n",
    "    word_embeddings = []\n",
    "    for token, embedding in zip(tokens, contextual_embeddings[0]):\n",
    "        word_embeddings.append((token, embedding))\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = []\n",
    "    for token, embedding in word_embeddings:\n",
    "        similarity = cosine_similarity(target_embedding.unsqueeze(0), embedding.unsqueeze(0)).item()\n",
    "        similarities.append((token, similarity))\n",
    "    \n",
    "    top_entries = []\n",
    "    seen_words = set()\n",
    "    for token, similarity in similarities:\n",
    "        if token not in seen_words:\n",
    "            # Add the current entry to the heap\n",
    "            heapq.heappush(top_entries, (similarity, token))\n",
    "            seen_words.add(token)\n",
    "\n",
    "            # If the heap exceeds size n, remove the smallest entry\n",
    "            if len(top_entries) > top_n:\n",
    "                # Remove the smallest entry and also remove it from the seen set\n",
    "                removed_similarity, removed_token = heapq.heappop(top_entries)\n",
    "                seen_words.remove(removed_token)\n",
    "\n",
    "    # Extract the entries from the heap and sort them in descending order\n",
    "    top_entries.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    return top_entries\n",
    "\n",
    "# Function to read text from a file\n",
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Path to the .txt file\n",
    "file_path = 'data/A10010_short.txt'\n",
    "\n",
    "# Read the text from the file\n",
    "text = read_text_from_file(file_path)\n",
    "\n",
    "# Target word to find similarities with\n",
    "target_word = \"natiue\"\n",
    "\n",
    "# Find and print top 10 similar words\n",
    "top_similar_words = find_top_similar_words(target_word, text, tokenizer, model, top_n=10)\n",
    "print(f\"Top 10 words most similar to '{target_word}':\")\n",
    "for word, similarity in top_similar_words:\n",
    "    print(f\"{similarity}: {word}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
