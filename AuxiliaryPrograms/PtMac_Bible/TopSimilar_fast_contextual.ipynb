{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Arial; font-size: 14pt;\"><b>Finding Top Similar Words</b></span><br>\n",
    "Author: Lucas Ma\n",
    "\n",
    "\n",
    "This version of finding the top n words with the highest cosine similarity is able to take into account the contextual information, and it could do so in a strikingly efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Arial; font-size: 11pt;\"><b>Note:</b></span><br>\n",
    "The following code should not be run until you run the pretrainMac.ipynb--it will help you train a fine-tuned MacBERTh locally and store it in a folder (which will be created) called fine-tuned-MacBERTh, and you do not have to worry about the trained model being too large to be pushed onto GitHub. The folder that contains the model has been ignored by Git, which can be shown in the file .gitignore.\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">Essentially, go run the program retrainMac.ipynb and come back to run the following code. Feel free to git add or git commit or git push as per normal.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ./fine-tuned-MacBERTh and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words most similar to 'work':\n",
      "0.5531777739524841: #\n",
      "0.5477403402328491: of\n",
      "0.54087895154953: when\n",
      "0.5402771830558777: #\n",
      "0.5390589833259583: therefore\n",
      "0.538948118686676: made\n",
      "0.5377960205078125: have\n",
      "0.5375989675521851: may\n",
      "0.5355035066604614: it\n",
      "0.533892810344696: ,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import heapq\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = './fine-tuned-MacBERTh'  # Path to your fine-tuned model\n",
    "# model_name = \"emanjavacas/MacBERTh\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to get contextual embeddings for a chunk of text\n",
    "def get_contextual_embeddings(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state, inputs\n",
    "\n",
    "# Function to split text into chunks of a given size\n",
    "def split_text_into_chunks(text, tokenizer, chunk_size=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = [' '.join(tokens[i:i + chunk_size]) for i in range(0, len(tokens), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Function to get the embedding for a specific word\n",
    "def get_word_embedding(word, tokenizer, model):\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\", padding='max_length', max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    return embeddings.squeeze()\n",
    "\n",
    "def find_top_similar_words(target_word, text, tokenizer, model, top_n=10):\n",
    "    # Split the text into chunks\n",
    "    chunks = split_text_into_chunks(text, tokenizer)\n",
    "\n",
    "    # Tokenize and get embedding for the target word\n",
    "    target_embedding = get_word_embedding(target_word, tokenizer, model)\n",
    "\n",
    "    # Collect similarities across all chunks\n",
    "    similarities = []\n",
    "    for chunk in chunks:\n",
    "        contextual_embeddings, inputs = get_contextual_embeddings(chunk, tokenizer, model)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        \n",
    "        # Find embeddings for each word in context\n",
    "        for i, (token, embedding) in enumerate(zip(tokens, contextual_embeddings[0])):\n",
    "            if token in tokenizer.all_special_tokens:\n",
    "                continue\n",
    "            similarity = cosine_similarity(target_embedding.unsqueeze(0), embedding.unsqueeze(0)).item()\n",
    "            similarities.append((token, similarity))\n",
    "    \n",
    "    # Use a heap to find the top N similar words\n",
    "    top_entries = []\n",
    "    #seen_words = set()\n",
    "    for token, similarity in similarities:\n",
    "        #if token not in seen_words:\n",
    "            heapq.heappush(top_entries, (similarity, token))\n",
    "            #seen_words.add(token)\n",
    "\n",
    "            if len(top_entries) > top_n:\n",
    "                removed_similarity, removed_token = heapq.heappop(top_entries)\n",
    "                #seen_words.remove(removed_token)\n",
    "\n",
    "    top_entries.sort(reverse=True, key=lambda x: x[0])\n",
    "    return top_entries\n",
    "\n",
    "# Function to read text from a file\n",
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Path to the .txt file\n",
    "file_path = 'data/A10010.P4 copy.txt'\n",
    "\n",
    "# Read the text from the file\n",
    "text = read_text_from_file(file_path)\n",
    "\n",
    "# Target word to find similarities with\n",
    "target_word = \"work\"\n",
    "\n",
    "# Find and print top 10 similar words\n",
    "top_similar_words = find_top_similar_words(target_word, text, tokenizer, model, top_n=10)\n",
    "print(f\"Top 10 words most similar to '{target_word}':\")\n",
    "for similarity, word in top_similar_words:\n",
    "    print(f\"{similarity}: {word}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
