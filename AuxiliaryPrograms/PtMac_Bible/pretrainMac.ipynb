{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Arial; font-size: 14pt;\"><b>Intermediate Training and Fine-tuning of BERT on Geneva Bible</b></span><br><br>\n",
    "<span style=\"font-family: Arial; font-size: 10pt;\">Author: Lucas Ma</span><br><br>\n",
    "<span style=\"font-family: Arial; font-size: 10pt;\"><b>Edit History:</b></span>\\\n",
    "<span style=\"font-family: Arial; font-size: 10pt;\">Jerry Zou (Jun 15)<br>Lucas Ma (Jun 16)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1af3d3aa044c8fb053700b69273655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5823884002cc41378558ff3e48d46279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a06ce4c67a04e3f8824296e7915c14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10.205, 'train_samples_per_second': 0.294, 'train_steps_per_second': 0.294, 'train_loss': 0.8662434418996176, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-MacBERTh/tokenizer_config.json',\n",
       " './fine-tuned-MacBERTh/special_tokens_map.json',\n",
       " './fine-tuned-MacBERTh/vocab.txt',\n",
       " './fine-tuned-MacBERTh/added_tokens.json',\n",
       " './fine-tuned-MacBERTh/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "# Step 1: Load Pre-trained Model and Tokenizer\n",
    "modelName = \"emanjavacas/MacBERTh\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModelForMaskedLM.from_pretrained(modelName)\n",
    "\n",
    "# Step 2: Prepare the Dataset\n",
    "def load_and_tokenize_dataset(file_path, tokenizer, block_size=128):\n",
    "    # Read lines from the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Create a dataset from lines\n",
    "    dataset = Dataset.from_dict({\"text\": lines})\n",
    "    \n",
    "    # Tokenize the dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=block_size)\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    return tokenized_dataset\n",
    "\n",
    "bible_text = \"data/bible_full_text.txt\"\n",
    "virginia_docs = \"data/A10010.txt\"\n",
    "\n",
    "# Load and tokenize datasets\n",
    "bible_dataset = load_and_tokenize_dataset(bible_text, tokenizer)\n",
    "virginia_dataset = load_and_tokenize_dataset(virginia_docs, tokenizer)\n",
    "\n",
    "# Concatenate the datasets\n",
    "#combined_dataset = DatasetDict({\"train\": torch.utils.data.ConcatDataset([bible_dataset, virginia_dataset])})\n",
    "combined_dataset = bible_dataset\n",
    "combined_dataset = DatasetDict({\"train\": combined_dataset})\n",
    "\n",
    "# Step 3: Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Step 4: Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Step 5: Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=combined_dataset[\"train\"]\n",
    ")\n",
    "\n",
    "# Step 6: Train\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./fine-tuned-MacBERTh')\n",
    "tokenizer.save_pretrained('./fine-tuned-MacBERTh')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: another approach to training the Geneva Bible through using CSV instead of TXT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In the beginning God created the heauen and the earth.', 'And the earth was without forme and voide, and darkenesse was vpon the deepe, and the Spirit of God moued vpon ye waters.', 'Then God saide, Let there be light: And there was light.', 'And God sawe the light that it was good, and God separated the light from the darkenes.', 'And God called the light, Day, and the darkenes, he called Night. So the euening and the morning were the first day.', 'Againe God saide, Let there be a firmament in the middes of the waters: and let it separate the waters from the waters.', 'Then God made the firmament, and separated the waters, which were vnder the firmament, from the waters which were aboue the firmament; it was so.', 'And God called the firmament Heauen. So the Euening and the morning were the seconde day.', 'God saide againe, Let the waters vnder the heauen be gathered into one place, and let the dry land appeare; it was so.', 'And God called the dry land, Earth, and he called the gathering together of the waters, Seas: and God sawe that it was good.']\n"
     ]
    }
   ],
   "source": [
    "# Process Geneva Bible CSV\n",
    "csvFilePath = \"./data/shortened_bible.csv\"\n",
    "dataFrame = pd.read_csv(csvFilePath)\n",
    "# dataFrame.head(10)\n",
    "verses = dataFrame[\"Text\"].tolist()\n",
    "print(verses[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasma/miniforge3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the sentence embeddings: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"emanjavacas/MacBERTh\")\n",
    "model = AutoModel.from_pretrained(\"emanjavacas/MacBERTh\")\n",
    "\n",
    "sentences = [\"Example sentence for dimension checking.\"]\n",
    "\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "sentence_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "print(f\"Shape of the sentence embeddings: {sentence_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used to fine tune MacBERTh on Bible saved in .csv. By Lucas Ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasma/miniforge3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61aa41d2b4314b4a92e6f969074cffbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11664 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4583, 'grad_norm': 24.043272018432617, 'learning_rate': 4.7856652949245545e-05, 'epoch': 0.13}\n",
      "{'loss': 1.4668, 'grad_norm': 18.28766441345215, 'learning_rate': 4.571330589849109e-05, 'epoch': 0.26}\n",
      "{'loss': 1.4698, 'grad_norm': 17.81636619567871, 'learning_rate': 4.3569958847736625e-05, 'epoch': 0.39}\n",
      "{'loss': 1.4387, 'grad_norm': 17.122652053833008, 'learning_rate': 4.142661179698217e-05, 'epoch': 0.51}\n",
      "{'loss': 1.4443, 'grad_norm': 12.819314002990723, 'learning_rate': 3.928326474622771e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3786, 'grad_norm': 19.872961044311523, 'learning_rate': 3.7139917695473254e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3927, 'grad_norm': 11.46866226196289, 'learning_rate': 3.49965706447188e-05, 'epoch': 0.9}\n",
      "{'loss': 1.4125, 'grad_norm': 25.160419464111328, 'learning_rate': 3.285322359396434e-05, 'epoch': 1.03}\n",
      "{'loss': 1.2937, 'grad_norm': 18.157798767089844, 'learning_rate': 3.0709876543209876e-05, 'epoch': 1.16}\n",
      "{'loss': 1.2929, 'grad_norm': 18.392080307006836, 'learning_rate': 2.856652949245542e-05, 'epoch': 1.29}\n",
      "{'loss': 1.2592, 'grad_norm': 20.636302947998047, 'learning_rate': 2.6423182441700962e-05, 'epoch': 1.41}\n",
      "{'loss': 1.286, 'grad_norm': 17.67131233215332, 'learning_rate': 2.4279835390946505e-05, 'epoch': 1.54}\n",
      "{'loss': 1.2487, 'grad_norm': 29.542827606201172, 'learning_rate': 2.2136488340192045e-05, 'epoch': 1.67}\n",
      "{'loss': 1.2775, 'grad_norm': 13.455879211425781, 'learning_rate': 1.9993141289437588e-05, 'epoch': 1.8}\n",
      "{'loss': 1.2414, 'grad_norm': 14.468206405639648, 'learning_rate': 1.784979423868313e-05, 'epoch': 1.93}\n",
      "{'loss': 1.1799, 'grad_norm': 26.813501358032227, 'learning_rate': 1.570644718792867e-05, 'epoch': 2.06}\n",
      "{'loss': 1.126, 'grad_norm': 17.97340202331543, 'learning_rate': 1.3563100137174212e-05, 'epoch': 2.19}\n",
      "{'loss': 1.1262, 'grad_norm': 15.805026054382324, 'learning_rate': 1.1419753086419753e-05, 'epoch': 2.31}\n",
      "{'loss': 1.1296, 'grad_norm': 13.876338958740234, 'learning_rate': 9.276406035665296e-06, 'epoch': 2.44}\n",
      "{'loss': 1.1463, 'grad_norm': 12.557714462280273, 'learning_rate': 7.133058984910837e-06, 'epoch': 2.57}\n",
      "{'loss': 1.1224, 'grad_norm': 14.345693588256836, 'learning_rate': 4.989711934156379e-06, 'epoch': 2.7}\n",
      "{'loss': 1.0722, 'grad_norm': 19.71721839904785, 'learning_rate': 2.8463648834019207e-06, 'epoch': 2.83}\n",
      "{'loss': 1.1034, 'grad_norm': 24.326345443725586, 'learning_rate': 7.030178326474623e-07, 'epoch': 2.96}\n",
      "{'train_runtime': 10659.4071, 'train_samples_per_second': 8.753, 'train_steps_per_second': 1.094, 'train_loss': 1.2739349019870836, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-MacBERTh/tokenizer_config.json',\n",
       " './fine-tuned-MacBERTh/special_tokens_map.json',\n",
       " './fine-tuned-MacBERTh/vocab.txt',\n",
       " './fine-tuned-MacBERTh/added_tokens.json',\n",
       " './fine-tuned-MacBERTh/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/genevaBible.csv')\n",
    "\n",
    "# Initialize the tokenizer for MacBERTh\n",
    "modelName = \"emanjavacas/MacBERTh\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModelForMaskedLM.from_pretrained(modelName)\n",
    "\n",
    "# Tokenize the texts\n",
    "texts = df['Text'].tolist()\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Create a Dataset object\n",
    "dataset = Dataset.from_dict(encodings)\n",
    "\n",
    "# Define the data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "trainer.save_model('./fine-tuned-MacBERTh')\n",
    "tokenizer.save_pretrained('./fine-tuned-MacBERTh')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
