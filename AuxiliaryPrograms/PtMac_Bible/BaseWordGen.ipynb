{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Arial; font-size: 14pt;\"><b>Finding Top Similar Words as Base Words</b></span><br>\n",
    "Author: Lucas Ma<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lucasma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/Users/lucasma/miniforge3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import string, heapq, numpy as np\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "import os, json\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Make sure to download the punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "model_name = \"emanjavacas/MacBERTh\"\n",
    "# model_name = \"fine-tuned-MacBERTh\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################\n",
      "Top 50 words most similar to 'money':\n",
      "1.0000001192092896: money\n",
      "0.933540403842926: wealth\n",
      "0.9297288060188293: silver\n",
      "0.9293482899665833: folly\n",
      "0.9277276992797852: debt\n",
      "0.925916850566864: favour\n",
      "0.9251276850700378: drunken\n",
      "0.9249783754348755: punishment\n",
      "0.9233909845352173: worthy\n",
      "0.9230401515960693: doctrine\n",
      "0.9217734336853027: treasure\n",
      "0.9207563996315002: property\n",
      "0.9207444190979004: reason\n",
      "0.920539379119873: plenty\n",
      "0.9201928377151489: portion\n",
      "0.9197781682014465: buy\n",
      "0.9195746779441833: penny\n",
      "0.9193478226661682: river\n",
      "0.9188104867935181: fight\n",
      "0.9187880158424377: wicked\n",
      "0.9186394810676575: journey\n",
      "0.9173088669776917: misery\n",
      "0.9171804189682007: shilling\n",
      "0.9166380167007446: building\n",
      "0.9166153073310852: build\n",
      "0.9160724878311157: drunkenness\n",
      "0.9160028696060181: water\n",
      "0.9159512519836426: people\n",
      "0.9156801104545593: price\n",
      "0.9154247045516968: happy\n",
      "0.9152998328208923: duty\n",
      "0.9148253798484802: ceremony\n",
      "0.9147508144378662: merchant\n",
      "0.9146800637245178: number\n",
      "0.9144884943962097: reckon\n",
      "0.9143115282058716: pay\n",
      "0.9142728447914124: martyr\n",
      "0.9140658974647522: metal\n",
      "0.9137795567512512: confession\n",
      "0.9134802222251892: suffer\n",
      "0.9134776592254639: slander\n",
      "0.9133360981941223: commendation\n",
      "0.9128735065460205: farthing\n",
      "0.9128731489181519: gold\n",
      "0.9126282334327698: dwell\n",
      "0.9125433564186096: garment\n",
      "0.9124430418014526: bond\n",
      "0.9124263525009155: colour\n",
      "0.9122477769851685: company\n",
      "0.9121819734573364: difficult\n",
      "######################################################\n",
      "Top 50 words most similar to 'christ':\n",
      "1.0000001192092896: christ\n",
      "0.9289001226425171: repentance\n",
      "0.92617267370224: scripture\n",
      "0.9253600835800171: solomon\n",
      "0.9241600632667542: christian\n",
      "0.918251097202301: child\n",
      "0.9165892601013184: son\n",
      "0.9160637259483337: king\n",
      "0.9158390164375305: david\n",
      "0.9156680107116699: grace\n",
      "0.9148444533348083: jesus\n",
      "0.9134332537651062: patience\n",
      "0.913020133972168: comfort\n",
      "0.9130071997642517: samuel\n",
      "0.912959098815918: temple\n",
      "0.9127172231674194: necessity\n",
      "0.911978006362915: price\n",
      "0.9112080335617065: prayer\n",
      "0.9108568429946899: miserable\n",
      "0.9108403921127319: danger\n",
      "0.9104588031768799: heaven\n",
      "0.9098169207572937: suffer\n",
      "0.9095253348350525: satan\n",
      "0.9091469049453735: justification\n",
      "0.9088044166564941: gospel\n",
      "0.9085184335708618: service\n",
      "0.9082004427909851: priest\n",
      "0.9081084728240967: speak\n",
      "0.9078233242034912: paul\n",
      "0.9077991247177124: commission\n",
      "0.9074833393096924: lamb\n",
      "0.906865119934082: justice\n",
      "0.906318187713623: pray\n",
      "0.9061145782470703: moses\n",
      "0.905985414981842: god\n",
      "0.9053190350532532: sickness\n",
      "0.9050976634025574: fruit\n",
      "0.904732882976532: house\n",
      "0.9043567776679993: john\n",
      "0.9040315747261047: psalm\n",
      "0.9038213491439819: honey\n",
      "0.9029844403266907: knowledge\n",
      "0.9029439687728882: punishment\n",
      "0.9028878808021545: sinner\n",
      "0.9028795957565308: consider\n",
      "0.9024986028671265: cross\n",
      "0.9023177623748779: peter\n",
      "0.9022934436798096: cause\n",
      "0.9021506309509277: scarlet\n",
      "0.9021021723747253: minister\n",
      "######################################################\n",
      "Top 50 words most similar to 'light':\n",
      "1.0: light\n",
      "0.9249585866928101: delight\n",
      "0.9151461720466614: water\n",
      "0.9139874577522278: gift\n",
      "0.9104140400886536: god\n",
      "0.9083282947540283: joy\n",
      "0.908023476600647: night\n",
      "0.9079617857933044: fight\n",
      "0.9076228141784668: rest\n",
      "0.9061284065246582: duty\n",
      "0.9055253863334656: religion\n",
      "0.9046741724014282: willing\n",
      "0.9046244621276855: weight\n",
      "0.9036213755607605: strong\n",
      "0.9034899473190308: touch\n",
      "0.9033094644546509: sight\n",
      "0.9030774831771851: gold\n",
      "0.9028913974761963: pure\n",
      "0.9021714329719543: trouble\n",
      "0.90192711353302: glorious\n",
      "0.9018715023994446: liberty\n",
      "0.9015333652496338: filthy\n",
      "0.9014536738395691: health\n",
      "0.9011422395706177: friend\n",
      "0.900532603263855: motion\n",
      "0.9005217552185059: perfection\n",
      "0.9000769257545471: sun\n",
      "0.8999814391136169: thought\n",
      "0.8995835185050964: plant\n",
      "0.8994908928871155: truth\n",
      "0.8994199633598328: glory\n",
      "0.899336040019989: comfort\n",
      "0.8992680907249451: direct\n",
      "0.8990057110786438: fire\n",
      "0.8988109827041626: love\n",
      "0.8987813591957092: journey\n",
      "0.8983240723609924: coal\n",
      "0.8982520699501038: prayer\n",
      "0.8980746865272522: repent\n",
      "0.8980499505996704: david\n",
      "0.8979916572570801: name\n",
      "0.8979660868644714: fault\n",
      "0.8979411125183105: tender\n",
      "0.8978779315948486: repentance\n",
      "0.8973562121391296: right\n",
      "0.8973467946052551: death\n",
      "0.8966612815856934: goodness\n",
      "0.8965493440628052: willingness\n",
      "0.8965353965759277: heaven\n",
      "0.8964695334434509: distinction\n",
      "######################################################\n",
      "Top 50 words most similar to 'darkness':\n",
      "1.0: darkness\n",
      "0.9507797956466675: dark\n",
      "0.9175273180007935: weakness\n",
      "0.91238933801651: sinful\n",
      "0.90807044506073: justice\n",
      "0.9063767790794373: sweetness\n",
      "0.9058458805084229: disease\n",
      "0.9050238728523254: vanity\n",
      "0.9035192728042603: righteousness\n",
      "0.9027547240257263: madness\n",
      "0.9017159938812256: sickness\n",
      "0.9014354944229126: wickedness\n",
      "0.9007238149642944: travel\n",
      "0.8983346223831177: boldness\n",
      "0.8982587456703186: deceit\n",
      "0.8978449702262878: rich\n",
      "0.8961009383201599: destruction\n",
      "0.8951613306999207: thief\n",
      "0.8948741555213928: bondage\n",
      "0.8946062326431274: fear\n",
      "0.8944122195243835: conviction\n",
      "0.8940891623497009: devil\n",
      "0.8939964771270752: goodness\n",
      "0.8939719796180725: horror\n",
      "0.893578827381134: food\n",
      "0.8933351635932922: ruin\n",
      "0.8924180269241333: grace\n",
      "0.892375111579895: burn\n",
      "0.892349898815155: mischief\n",
      "0.8922999501228333: natural\n",
      "0.8913435935974121: heat\n",
      "0.891250729560852: lazy\n",
      "0.8912150859832764: thorn\n",
      "0.8906430602073669: happiness\n",
      "0.8905642628669739: drunkenness\n",
      "0.8902914524078369: heaven\n",
      "0.8902488350868225: rock\n",
      "0.8901594877243042: war\n",
      "0.8901400566101074: sincerity\n",
      "0.8899798393249512: guilt\n",
      "0.8897705078125: sin\n",
      "0.8897008299827576: adultery\n",
      "0.8893283605575562: solomon\n",
      "0.8892119526863098: zeal\n",
      "0.8889540433883667: sensual\n",
      "0.888259768486023: greatness\n",
      "0.8878855109214783: law\n",
      "0.8878390789031982: wisdom\n",
      "0.887363612651825: dirt\n",
      "0.887328565120697: dead\n",
      "######################################################\n",
      "Top 50 words most similar to 'clothes':\n",
      "0.7777217626571655: mathem\n",
      "0.7714601159095764: especial\n",
      "0.770417332649231: independence\n",
      "0.7658366560935974: resemblance\n",
      "0.7648184299468994: values\n",
      "0.7643887400627136: apprehending\n",
      "0.763584315776825: expense\n",
      "0.7624803185462952: expedite\n",
      "0.7614074349403381: epitome\n",
      "0.7598892450332642: electuary\n",
      "0.7591184377670288: inconsider\n",
      "0.7590444087982178: elegant\n",
      "0.7575441598892212: malach\n",
      "0.7572156190872192: intimation\n",
      "0.7566043138504028: bristol\n",
      "0.7565540075302124: passenger\n",
      "0.7565173506736755: augmentation\n",
      "0.7559433579444885: wary\n",
      "0.7558696866035461: immutable\n",
      "0.755506157875061: assertion\n",
      "0.7554905414581299: braw\n",
      "0.7553852200508118: vz\n",
      "0.7539854049682617: sympt\n",
      "0.7530556321144104: bese\n",
      "0.7524753212928772: anon\n",
      "0.7516970038414001: doubtless\n",
      "0.7514822483062744: faulty\n",
      "0.7510018348693848: accommodate\n",
      "0.750856876373291: beforehand\n",
      "0.7507950067520142: philipp\n",
      "0.7506711483001709: heretofore\n",
      "0.7500698566436768: considerable\n",
      "0.7497177720069885: intercess\n",
      "0.7493916153907776: withheld\n",
      "0.7489609718322754: obvious\n",
      "0.7489151358604431: revers\n",
      "0.7488990426063538: admitt\n",
      "0.7488868832588196: equivalent\n",
      "0.7486221194267273: irrecover\n",
      "0.7486073970794678: predomin\n",
      "0.7485802173614502: laban\n",
      "0.7481854557991028: unac\n",
      "0.7473766207695007: torch\n",
      "0.7468171119689941: chrysostom\n",
      "0.7466323375701904: exempl\n",
      "0.7465429306030273: soever\n",
      "0.7462322115898132: adopt\n",
      "0.7461432218551636: forasmuch\n",
      "0.7460848689079285: pernicious\n",
      "0.7458736896514893: ambition\n",
      "######################################################\n",
      "Top 50 words most similar to 'naked':\n",
      "1.0000001192092896: naked\n",
      "0.9232437610626221: lame\n",
      "0.9163221120834351: ragged\n",
      "0.9133532047271729: comely\n",
      "0.9123278260231018: glorious\n",
      "0.9117134809494019: wicked\n",
      "0.9116715788841248: garment\n",
      "0.9094234108924866: daughter\n",
      "0.9091277122497559: cover\n",
      "0.9083554148674011: wretched\n",
      "0.9081321954727173: miserable\n",
      "0.9076624512672424: beautiful\n",
      "0.907480776309967: swine\n",
      "0.9069642424583435: drunken\n",
      "0.9060892462730408: figurative\n",
      "0.9059942960739136: belly\n",
      "0.9056615233421326: scarlet\n",
      "0.9051324725151062: regeneration\n",
      "0.9047040343284607: righteous\n",
      "0.9046521782875061: viper\n",
      "0.9045074582099915: doubtful\n",
      "0.9044093489646912: clad\n",
      "0.9041360020637512: trouble\n",
      "0.9039440751075745: ashamed\n",
      "0.9039133191108704: monstrous\n",
      "0.9038190245628357: palace\n",
      "0.9037452340126038: rebellious\n",
      "0.9035733342170715: robe\n",
      "0.9034125804901123: troublesome\n",
      "0.9033952355384827: deformity\n",
      "0.9030781984329224: simple\n",
      "0.9027879238128662: profitable\n",
      "0.9026623964309692: fountain\n",
      "0.902459442615509: shilling\n",
      "0.9023033976554871: stately\n",
      "0.9022814035415649: negligence\n",
      "0.9022603034973145: commendable\n",
      "0.9019367694854736: negligent\n",
      "0.9017314910888672: wooden\n",
      "0.9016909599304199: hungry\n",
      "0.9016656875610352: bondage\n",
      "0.9015688300132751: safe\n",
      "0.9014215469360352: stubborn\n",
      "0.9012438058853149: sinner\n",
      "0.9011910557746887: guilty\n",
      "0.9011144042015076: countenance\n",
      "0.9009157419204712: generation\n",
      "0.9008421897888184: preacher\n",
      "0.9006565809249878: painter\n",
      "0.8998535871505737: dangerous\n"
     ]
    }
   ],
   "source": [
    "def read_sentence_document(document_path):\n",
    "    encodings = ['utf-8', 'latin-1', 'cp1252']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(document_path, 'r', encoding=encoding) as f:\n",
    "                text = f.read()\n",
    "            break  # Exit the loop if no exception was raised\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    else:\n",
    "        raise UnicodeDecodeError(\"Failed to read the file with any of the tried encodings.\")\n",
    "        \n",
    "    text = text.lower()\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Further split the sentences if there are more than 450 words\n",
    "    split_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        while len(words) > 400:\n",
    "            split_sentences.append(' '.join(words[:400]))\n",
    "            words = words[400:]\n",
    "        split_sentences.append(' '.join(words))\n",
    "\n",
    "    # Strip whitespace and filter out empty sentences\n",
    "    final_sentences = [s.strip() for s in split_sentences if s.strip()]\n",
    "    \n",
    "    return final_sentences\n",
    "\n",
    "\n",
    "# Function to get word embeddings\n",
    "def get_word_embedding(chunks, tokenizer, model):\n",
    "    word_times = {}\n",
    "    word_embeddings = {}\n",
    "    for chunk in chunks:\n",
    "\n",
    "        inputs = tokenizer(chunk, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        for i, word in enumerate(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])):\n",
    "            if word not in word_times:\n",
    "                word_times[word] = 1\n",
    "            else:\n",
    "                word_times[word]+=1\n",
    "            if word not in word_embeddings:\n",
    "                word_embeddings[word] = embeddings[0, i, :].numpy()\n",
    "            else:\n",
    "                word_embeddings[word] = (word_embeddings[word] * (word_times[word]-1) + embeddings[0, i, :].numpy()) / word_times[word]\n",
    "    return word_embeddings\n",
    "\n",
    "def get_single_embedding(word, tokenizer, model):\n",
    "    # Tokenize the input word\n",
    "    inputs = tokenizer(word, return_tensors='pt')\n",
    "    \n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Pass the tokenized input through the model\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the embeddings\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    \n",
    "    # Convert token IDs back to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Store the embeddings in a dictionary\n",
    "    word_embeddings = {}\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == word:\n",
    "            word_embeddings[token] = embeddings[0, i, :].numpy()\n",
    "            break\n",
    "    \n",
    "    return word_embeddings\n",
    "\n",
    "\n",
    "\n",
    "# Find the substitute words for the original keyword, by iterating over the standard word list\n",
    "def substitute_word(word):\n",
    "    json_dir = \"/Users/lucasma/Documents/The States/ECBC/Code/ECBCData2024/standardizedwords.json\"\n",
    "    with open (json_dir, \"r\") as f:\n",
    "        standardWord = json.load(f)\n",
    "    ret = []\n",
    "    for term, equals in standardWord.items():\n",
    "        for spell in equals:\n",
    "            if spell==word:\n",
    "                ret.append(term)\n",
    "    return ret\n",
    "\n",
    "def clean_embedding(embeddings):\n",
    "    clean = {}\n",
    "    for token, embedding in embeddings.items():\n",
    "        if (token in tokenizer.all_special_tokens) or (token.lower() in stop_words) or (token in string.punctuation) or (token.startswith('##')) or (token==\"â€¢\"):\n",
    "            continue\n",
    "        clean[token] = embedding\n",
    "    return clean\n",
    "\n",
    "# Main function\n",
    "def find_top_similar_words(target_words, sentences, tokenizer, model, top_n):\n",
    "    \n",
    "    embeddings = clean_embedding(get_word_embedding(sentences, tokenizer, model))\n",
    "    key_sim_word = {}\n",
    "\n",
    "    for keyWord in target_words:\n",
    "        presence = \"\"\n",
    "        wordFound = False\n",
    "\n",
    "        if keyWord in embeddings:\n",
    "            presence = keyWord\n",
    "            wordFound = True\n",
    "        else:\n",
    "            for equal in substitute_word(keyWord):\n",
    "                wordFound = wordFound or equal in embeddings\n",
    "                if wordFound:\n",
    "                    presence = equal\n",
    "                    break\n",
    "        \n",
    "        if not wordFound:\n",
    "            key_embedding =  get_single_embedding(keyWord, tokenizer, model)[keyWord]\n",
    "        else:\n",
    "            key_embedding = embeddings[presence]\n",
    "\n",
    "        similarities = {}\n",
    "\n",
    "        for token, embedding in embeddings.items():\n",
    "            similarities[token] = np.dot(embedding, key_embedding) / ((np.linalg.norm(embedding)) * (np.linalg.norm(key_embedding)))\n",
    "    \n",
    "        # Use a heap to find the top N similar words\n",
    "        top_entries = []\n",
    "\n",
    "        for token, similarity in similarities.items():\n",
    "                heapq.heappush(top_entries, (similarity, token))\n",
    "\n",
    "                if len(top_entries) > top_n:\n",
    "                    removed_sim, removed_t = heapq.heappop(top_entries)\n",
    "\n",
    "        top_entries.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "        key_sim_word[keyWord] = top_entries\n",
    "\n",
    "    return key_sim_word\n",
    "\n",
    "\n",
    "# Path to the .txt file\n",
    "file_path = '/Users/lucasma/Documents/The\\ States/ECBC/Code/ECBCData2024/data/VirginiaTotal.txt'\n",
    "# file_path = \"data/A10010_cleaned.txt\"\n",
    "\n",
    "sentences = read_sentence_document(file_path)\n",
    "\n",
    "# Target word to find similarities with\n",
    "target_words = [\"money\", \"christ\", \"light\", \"darkness\", \"clothes\", \"naked\"]\n",
    "\n",
    "# Find and print top 10 similar words\n",
    "top_similar_words = find_top_similar_words(target_words, sentences, tokenizer, model, 50)\n",
    "\n",
    "for word, words in top_similar_words.items():\n",
    "    print(\"######################################################\")\n",
    "    print(f\"Top 50 words most similar to '{word}':\")\n",
    "    for (sim, relWord) in words:\n",
    "        print(f\"{relWord}: {sim}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
