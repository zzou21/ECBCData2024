{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT training practice test subject: using Keras and Tensorflow for text classification instead of Transformers library\n",
    "\n",
    "Author: Jerry Zou\n",
    "\n",
    "To install completel tensorflow files: \\\n",
    "pip install -q tensorflow==2.3.0  \\\n",
    "git clone --depth 1 -b v2.3.0 https://github.com/tensorflow/models.git  \\\n",
    "pip install -Uqr models/official/requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from transformers import AutoTokenizer, TFBertModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# If needed: \n",
    "# import kagglehub\n",
    "# # Download latest version of encoder\n",
    "# path = kagglehub.model_download(\"tensorflow/bert/tensorFlow2/en-uncased-l-12-h-768-a-12\")\n",
    "# print(\"Path to model files:\", path)\n",
    "\n",
    "# # Download latest version of preprocessor\n",
    "# path = kagglehub.model_download(\"tensorflow/bert/tensorFlow2/en-uncased-preprocess\")\n",
    "# print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_csv(\"/Users/Jerry/Desktop/KerasPracticeQuoraTrain.csv\")\n",
    "#EXAMPLE: dataFrame.groupby(\"target\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTraining, xTesting, yTraining, yTesting = train_test_split(dataFrame[\"question_text\"], dataFrame[\"target\"], test_size=0.2, stratify=dataFrame[\"target\"])\n",
    "# it is important to have stratify for imbalanced datasets\n",
    "\n",
    "#EXAMPLE: yTraining.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using keras layer\n",
    "bertPreprocessName = '/Users/Jerry/.cache/kagglehub/models/tensorflow/bert/tensorFlow2/en-uncased-preprocess/3'\n",
    "#main processing\n",
    "bertPreprocess = hub.KerasLayer(bertPreprocessName)\n",
    "#main encoder\n",
    "bertEncoder = hub.KerasLayer(\"/Users/Jerry/.cache/kagglehub/models/tensorflow/bert/tensorFlow2/en-uncased-l-12-h-768-a-12/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceEmbedding(sentences):\n",
    "    preprocessedText = bertPreprocess(sentences)\n",
    "    return bertEncoder(preprocessedText)[\"pooled_output\"]\n",
    "\n",
    "#EXAMPLE: getSentenceEmbedding([\"los angeles is a wonderdful city\", \"it is not quite good.\"])\n",
    "#EXAMPLE:\n",
    "# e = getSentenceEmbedding([\n",
    "#     \"banana\",\n",
    "#     \"grapes\",\n",
    "#     \"mango\",\n",
    "#     \"jeff bezos\",\n",
    "#     \"elon musk\",\n",
    "#     \"bill gates\"\n",
    "# ])\n",
    "# cosine_similarity([e[4]],[e[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLayer = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text\")\n",
    "preprocessedText = bertPreprocess(inputLayer)\n",
    "outputs = bertEncoder(preprocessedText)\n",
    "layer = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs[\"pooled_output\"])\n",
    "layer = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output\")(layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inputLayer], outputs = [layer])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "#TRAINING:\n",
    "model.fit(xTraining, yTraining, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(xTesting, yTesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULT:\n",
    "# input content and receive result. Over 0.5 (or 50%) would be considered as matching.\n",
    "review = [\"content content sentence\", \"content sentence content\"]\n",
    "model.predict(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "BELOW ARE FAILED CODE THAT I'M KEEPING FOR FUTURE REFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib as plt\n",
    "# import sys\n",
    "# sys.path.append(\"models\")\n",
    "# from official.nlp.data import classifier_data_lib\n",
    "# from official.nlp.bert import tokenization\n",
    "# from official.nlp import optimization\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from transformers import AutoTokenizer, TFBertModel\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Activation, Dense\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'bert-base-uncased'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# labelList = [0, 1]\n",
    "# maxSeqLength = 128\n",
    "\n",
    "# # Define the mapping function to tokenize text and convert label\n",
    "# def toFeatureMap(text, label):\n",
    "#     encoding = tokenizer(text.numpy().decode('utf-8'), truncation=True, padding='max_length', max_length=maxSeqLength, return_tensors='tf')\n",
    "    \n",
    "#     # Extract element from the batched tensors\n",
    "#     inputID = encoding['input_ids'][0]\n",
    "#     attentionMask = encoding['attention_mask'][0]\n",
    "#     tokenTypeID = encoding['token_type_ids'][0] if 'token_type_ids' in encoding else tf.zeros_like(input_ids)\n",
    "    \n",
    "#     labelID = tf.convert_to_tensor(label, dtype=tf.int32)\n",
    "#     return {'input_ids': inputID, 'attention_mask': attentionMask, 'token_type_ids': tokenTypeID}, labelID\n",
    "\n",
    "# # Wrap the function with tf.py_function to make it compatible with TensorFlow's graph execution\n",
    "# def toFeatureMapWrapper(text, label):\n",
    "#     return tf.py_function(\n",
    "#         toFeatureMap, \n",
    "#         inp=[text, label], \n",
    "#         Tout=(\n",
    "#             {\n",
    "#                 'input_ids': tf.int32,\n",
    "#                 'attention_mask': tf.int32,\n",
    "#                 'token_type_ids': tf.int32\n",
    "#             },\n",
    "#             tf.int32\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# #------\n",
    "\n",
    "# dataFrame = pd.read_csv(\"/Users/Jerry/Desktop/KerasPracticeQuoraTrain.csv\")\n",
    "# dataFrame.tail(20)\n",
    "# # qid - unique question identifier\n",
    "# # question_text - Quora question text\n",
    "# # target - a question labeled \"insincere\" has a value of 1, otherwise 0\n",
    "\n",
    "# #dataFrame.target.plot(kind=\"hist\", title=\"Target distribution\")\n",
    "\n",
    "\n",
    "\n",
    "# #Split into training and testing datasets\n",
    "# trainingData, remaining = train_test_split(dataFrame, train_size=0.005, stratify=dataFrame[\"target\"])\n",
    "# validationData, _ = train_test_split(remaining, random_state=45, train_size=0.0005, stratify=remaining[\"target\"])\n",
    "# #trainingData.shape, validationData.shape\n",
    "\n",
    "# #------\n",
    "\n",
    "# # Creating tensorflow dataset to create python iterables. These datasets have two values, so use \"for x, y in ...\" during iteration.\n",
    "# trainData = tf.data.Dataset.from_tensor_slices((trainingData[\"question_text\"].values, trainingData[\"target\"].values))\n",
    "# validData = tf.data.Dataset.from_tensor_slices((validationData[\"question_text\"].values, validationData[\"target\"].values))\n",
    "\n",
    "# #maps the data to prepare for training\n",
    "# trainData = trainData.map(toFeatureMapWrapper, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# validData = validData.map(toFeatureMapWrapper, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# for question, targetValue in trainData.take(1):\n",
    "#     print(question)\n",
    "#     print(targetValue)\n",
    "\n",
    "# # -------\n",
    "\n",
    "# # Preprocessing\n",
    "# trainBatchSize = 32\n",
    "# trainData = trainData.batch(trainBatchSize).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "# validData = validData.batch(trainBatchSize).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# modelName = 'bert-base-uncased'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "# model = TFBertModel.from_pretrained(modelName)\n",
    "\n",
    "# class BertClassifier(tf.keras.Model):\n",
    "#     def __init__(self, modelName):\n",
    "#         super(BertClassifier, self).__init__()\n",
    "#         self.bert = modelName\n",
    "#         self.dense = tf.keras.layers.Dense(2, activation='softmax')  # Assuming binary classification\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "#         inputIDs, attentionMask = inputs\n",
    "#         outputs = self.bert(inputIDs, attention_mask=attentionMask)\n",
    "#         pooled_output = outputs.pooler_output\n",
    "#         return self.dense(pooled_output)\n",
    "    \n",
    "# model = BertClassifier(modelName)\n",
    "\n",
    "#-----\n",
    "\n",
    "# # Preprocessing\n",
    "# trainBatchSize = 32\n",
    "# trainData = trainData.batch(trainBatchSize).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "# validData = validData.batch(trainBatchSize).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# modelName = 'bert-base-uncased'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "# model = TFBertModel.from_pretrained(modelName)\n",
    "\n",
    "# class BertClassifier(tf.keras.Model):\n",
    "#     def __init__(self, modelName):\n",
    "#         super(BertClassifier, self).__init__()\n",
    "#         self.bert = modelName\n",
    "#         self.dense = tf.keras.layers.Dense(2, activation='softmax')  # Assuming binary classification\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "#         inputIDs, attentionMask = inputs\n",
    "#         outputs = self.bert(inputIDs, attention_mask=attentionMask)\n",
    "#         pooled_output = outputs.pooler_output\n",
    "#         return self.dense(pooled_output)\n",
    "    \n",
    "# model = BertClassifier(modelName)\n",
    "\n",
    "\n",
    "# #------\n",
    "\n",
    "\n",
    "# text = \"Hi How are you doing?\"\n",
    "# # Displays each individual word\n",
    "# #tokenizer.tokenize(text)\n",
    "\n",
    "# # Displays each token's input ID\n",
    "# #tokenizer.encode(text)\n",
    "\n",
    "# #dispays input id, type id, and attention mask\n",
    "# tokenizer(text)\n",
    "\n",
    "# #-----\n",
    "# encodings = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "# input_ids = encodings['input_ids']\n",
    "# attention_mask = encodings['attention_mask']\n",
    "# print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from transformers import AutoTokenizer, TFBertModel\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Define the tokenizer\n",
    "# model_name = 'bert-base-uncased'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# labelList = [0, 1]  # Example label list\n",
    "# maxSeqLength = 128  # Example maximum sequence length\n",
    "\n",
    "# # Define the mapping function to tokenize text and convert label\n",
    "# def toFeatureMap(text, label):\n",
    "#     # Tokenize the text\n",
    "#     encoding = tokenizer(\n",
    "#         text.numpy().decode('utf-8'),\n",
    "#         truncation=True,\n",
    "#         padding='max_length',\n",
    "#         max_length=maxSeqLength,\n",
    "#         return_tensors='tf'\n",
    "#     )\n",
    "    \n",
    "#     # Extract the first (and only) element from the batched tensors\n",
    "#     input_ids = tf.convert_to_tensor(encoding['input_ids'][0], dtype=tf.int32)\n",
    "#     attention_mask = tf.convert_to_tensor(encoding['attention_mask'][0], dtype=tf.int32)\n",
    "#     token_type_ids = tf.convert_to_tensor(encoding['token_type_ids'][0] if 'token_type_ids' in encoding else tf.zeros_like(input_ids), dtype=tf.int32)\n",
    "    \n",
    "#     # Convert the label to its corresponding ID\n",
    "#     label_id = tf.convert_to_tensor(label, dtype=tf.int32)\n",
    "    \n",
    "#     return input_ids, attention_mask, token_type_ids, label_id\n",
    "\n",
    "# # Wrap the function with tf.py_function to make it compatible with TensorFlow's graph execution\n",
    "# def toFeatureMapWrapper(text, label):\n",
    "#     input_ids, attention_mask, token_type_ids, label_id = tf.py_function(\n",
    "#         toFeatureMap, \n",
    "#         inp=[text, label], \n",
    "#         Tout=[tf.int32, tf.int32, tf.int32, tf.int32]\n",
    "#     )\n",
    "#     input_ids.set_shape([maxSeqLength])\n",
    "#     attention_mask.set_shape([maxSeqLength])\n",
    "#     token_type_ids.set_shape([maxSeqLength])\n",
    "#     label_id.set_shape([])\n",
    "#     return {\n",
    "#         'input_ids': input_ids,\n",
    "#         'attention_mask': attention_mask,\n",
    "#         'token_type_ids': token_type_ids\n",
    "#     }, label_id\n",
    "\n",
    "# # Load the CSV file\n",
    "# dataFrame = pd.read_csv(\"/Users/Jerry/Desktop/KerasPracticeQuoraTrain.csv\")\n",
    "# print(dataFrame.tail(5))\n",
    "\n",
    "# # Split the data into training and validation sets\n",
    "# trainingData, remaining = train_test_split(dataFrame, train_size=0.005, stratify=dataFrame[\"target\"])\n",
    "# validationData, _ = train_test_split(remaining, random_state=45, train_size=0.0005, stratify=remaining[\"target\"])\n",
    "\n",
    "# # Create TensorFlow datasets from the training and validation data\n",
    "# trainData = tf.data.Dataset.from_tensor_slices((trainingData[\"question_text\"].values, trainingData[\"target\"].values))\n",
    "# validData = tf.data.Dataset.from_tensor_slices((validationData[\"question_text\"].values, validationData[\"target\"].values))\n",
    "\n",
    "# # Map the dataset using the mapping function\n",
    "# trainData = trainData.map(toFeatureMapWrapper, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# validData = validData.map(toFeatureMapWrapper, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# print(trainData)\n",
    "# print(validData)\n",
    "\n",
    "# # Batch and prefetch the datasets\n",
    "# trainBatchSize = 32\n",
    "# trainData = trainData.batch(trainBatchSize).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "# validData = validData.batch(trainBatchSize).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# # Define the model\n",
    "# class BertClassifier(tf.keras.Model):\n",
    "#     def __init__(self, modelName):\n",
    "#         super(BertClassifier, self).__init__()\n",
    "#         self.bert = TFBertModel.from_pretrained(modelName)  # Load BERT model here\n",
    "#         self.dense = tf.keras.layers.Dense(2, activation='softmax')  # Assuming binary classification\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         input_ids = inputs['input_ids']\n",
    "#         attention_mask = inputs['attention_mask']\n",
    "#         token_type_ids = inputs['token_type_ids']\n",
    "#         outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "#         pooled_output = outputs.pooler_output\n",
    "#         return self.dense(pooled_output)\n",
    "\n",
    "# # Initialize the model\n",
    "# model = BertClassifier(model_name)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(trainData, validation_data=validData, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
