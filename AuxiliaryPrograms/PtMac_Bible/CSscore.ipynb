{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Arial; font-size: 14pt;\"><b>Intermediate Training and Fine-tuning of BERT on Geneva Bible</b></span><br><br>\n",
    "<span style=\"font-family: Arial; font-size: 12pt;\">Author: Lucas Ma</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first chunk of code loads the pretrained model in the repo. Note: the directory of the trained model is purposefully untracked by git. Please do not attempt to track it by \"git add XXX\" as it prevents you from pushing successfully to the remote main branch i.e. our GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned MacBERTh model and tokenizer\n",
    "model_name = './fine-tuned-MacBERTh'  # Path to your fine-tuned model\n",
    "# model_name = \"emanjavacas/MacBERTh\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word, tokenizer, model):\n",
    "    # Tokenize the input word\n",
    "    inputs = tokenizer(word, return_tensors='pt')\n",
    "    \n",
    "    # Get the hidden states from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # Extract the last hidden state for the input token\n",
    "    hidden_states = outputs.hidden_states[-1]  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "    \n",
    "    # Get the embedding for the first token (the input word)\n",
    "    word_embedding = hidden_states[0, 1, :]  # Assuming the word is at position 1\n",
    "    return word_embedding\n",
    "\n",
    "# Example words\n",
    "word1 = \"god\"\n",
    "word2 = \"love\"\n",
    "\n",
    "# Get embeddings for the words\n",
    "embedding1 = get_word_embedding(word1, tokenizer, model)\n",
    "embedding2 = get_word_embedding(word2, tokenizer, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between 'god' and 'love': 0.950132429599762\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarity(embedding1, embedding2):\n",
    "    return cosine_similarity(embedding1.unsqueeze(0), embedding2.unsqueeze(0)).item()\n",
    "\n",
    "# Compute the cosine similarity between the embeddings\n",
    "cosine_sim = calculate_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine Similarity between '{word1}' and '{word2}': {cosine_sim}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
