{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Arial; font-size: 14pt;\"><b>Finding Top Similar Words</b></span><br>\n",
    "Author: Lucas Ma<br>\n",
    "Edited by Jerry Zou\n",
    "\n",
    "This version of finding the top n words with the highest cosine similarity is able to take into account the contextual information, and it could do so in a strikingly efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Arial; font-size: 11pt;\"><b>Note:</b></span><br>\n",
    "The following code should not be run until you run the pretrainMac.ipynb--it will help you train a fine-tuned MacBERTh locally and store it in a folder (which will be created) called fine-tuned-MacBERTh, and you do not have to worry about the trained model being too large to be pushed onto GitHub. The folder that contains the model has been ignored by Git, which can be shown in the file .gitignore.\n",
    "\n",
    "<span style=\"font-family: Arial; font-size: 11pt;\">Essentially, go run the program retrainMac.ipynb and come back to run the following code. Feel free to git add or git commit or git push as per normal.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch, string, json, heapq\n",
    "import numpy as np\n",
    "from torch.nn.functional import cosine_similarity\n",
    "# from docx import Document\n",
    "from tqdm import tqdm\n",
    "#import matplotlib.pyplot as plt\n",
    "#import networkx as nx\n",
    "#import seaborn as sns\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ./fine-tuned-MacBERTh and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words most similar to 'work':\n",
      "0.6188908219337463: of\n",
      "0.5968559384346008: his\n",
      "0.5933787226676941: us\n",
      "0.5915705561637878: made\n",
      "0.5914732813835144: comes\n",
      "0.5902810096740723: us\n",
      "0.589349627494812: my\n",
      "0.5881844758987427: and\n",
      "0.5880509614944458: #\n",
      "0.5873427391052246: is\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "model_name = './fine-tuned-MacBERTh'  # Path to your fine-tuned model\n",
    "# model_name = \"emanjavacas/MacBERTh\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to get contextual embeddings for a chunk of text\n",
    "def get_contextual_embeddings(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state, inputs\n",
    "\n",
    "# Function to split text into chunks of a given size\n",
    "def split_text_into_chunks(text, tokenizer, chunk_size=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = [' '.join(tokens[i:i + chunk_size]) for i in range(0, len(tokens), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Function to get the embedding for a specific word\n",
    "def get_word_embedding(word, tokenizer, model):\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\", padding='max_length', max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    return embeddings.squeeze()\n",
    "\n",
    "def find_top_similar_words(target_word, text, tokenizer, model, top_n=10):\n",
    "    # Split the text into chunks\n",
    "    chunks = split_text_into_chunks(text, tokenizer)\n",
    "\n",
    "    # Tokenize and get embedding for the target word\n",
    "    target_embedding = get_word_embedding(target_word, tokenizer, model)\n",
    "\n",
    "    # Collect similarities across all chunks\n",
    "    similarities = []\n",
    "    for chunk in chunks:\n",
    "        contextual_embeddings, inputs = get_contextual_embeddings(chunk, tokenizer, model)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        \n",
    "        # Find embeddings for each word in context\n",
    "        for i, (token, embedding) in enumerate(zip(tokens, contextual_embeddings[0])):\n",
    "            if token in tokenizer.all_special_tokens:\n",
    "                continue\n",
    "            similarity = cosine_similarity(target_embedding.unsqueeze(0), embedding.unsqueeze(0)).item()\n",
    "            similarities.append((token, similarity))\n",
    "    \n",
    "    # Use a heap to find the top N similar words\n",
    "    top_entries = []\n",
    "    #seen_words = set()\n",
    "    for token, similarity in similarities:\n",
    "        #if token not in seen_words:\n",
    "            heapq.heappush(top_entries, (similarity, token))\n",
    "            #seen_words.add(token)\n",
    "\n",
    "            if len(top_entries) > top_n:\n",
    "                removed_similarity, removed_token = heapq.heappop(top_entries)\n",
    "                #seen_words.remove(removed_token)\n",
    "\n",
    "    top_entries.sort(reverse=True, key=lambda x: x[0])\n",
    "    return top_entries\n",
    "\n",
    "# Function to read text from a file\n",
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Path to the .txt file\n",
    "file_path = 'data/A10010.P4 copy.txt'\n",
    "\n",
    "# Read the text from the file\n",
    "text = read_text_from_file(file_path)\n",
    "\n",
    "# Target word to find similarities with\n",
    "target_word = \"work\"\n",
    "\n",
    "# Find and print top 10 similar words\n",
    "top_similar_words = find_top_similar_words(target_word, text, tokenizer, model, top_n=10)\n",
    "print(f\"Top 10 words most similar to '{target_word}':\")\n",
    "for similarity, word in top_similar_words:\n",
    "    print(f\"{similarity}: {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Code below are merged code between Lucas's and Jerry's programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In progress\n",
    "\n",
    "# Lucas-Jerry Merged Code for Top Similar Words in a document\n",
    "modelName = \"emanjavacas/MacBERTh\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModel.from_pretrained(modelName)\n",
    "\n",
    "def preprocess_text(docx):\n",
    "    content = Document(docx)\n",
    "    lowercaseWords = []\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    for paragraph in content.paragraphs:\n",
    "        for run in paragraph.runs:\n",
    "            text = run.text\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                word = word.translate(translator)\n",
    "                lowercaseWords.append(word.lower())\n",
    "    joinedList = \" \".join(lowercaseWords)\n",
    "    return tokenizer.tokenize(joinedList)\n",
    "# FOR DEBUG: print(preprocess_text(\"/Users/Jerry/Desktop/test.docx\")[:40])\n",
    "\n",
    "def encode_text(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    #tokens = tokenizer.tokenize(text)\n",
    "    #combined_word = tokenizer.convert_tokens_to_string(tokens)\n",
    "    print(f\"Encoding word: {text}\")\n",
    "    return outputs.last_hidden_state.mean(dim=1)  # Use the mean of the hidden states as the embedding\n",
    "\n",
    "document_text = \"/Users/Jerry/Desktop/test.docx\"\n",
    "tokenizedText = preprocess_text(document_text)\n",
    "tokenTextEmbedding = {token: encode_text(token, tokenizer, model) for token in tokenizedText}\n",
    "\n",
    "keywords = [\"ciuilitie\", \"Sathan\", \"school\", \"instruction\"]\n",
    "tokenKeywordEmbeddings = {token: encode_text(token, tokenizer, model) for token in keywords}\n",
    "\n",
    "# for token, embedding in tokenTextEmbedding.items():\n",
    "#     print(f\"Token: {token}, Embedding: {embedding}\")\n",
    "\n",
    "\n",
    "def find_similar_words(keyword_embeddings, token_embeddings):\n",
    "    similar_words = {}\n",
    "    for keyword, keyword_emb in keyword_embeddings.items():\n",
    "        similarities = []\n",
    "        for token, token_emb in token_embeddings.items():\n",
    "            sim = cosine_similarity(keyword_emb, token_emb)\n",
    "            similarities.append((token, float(sim[0][0])))\n",
    "        # Sort by similarity score in descending order\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        similar_words[keyword] = similarities\n",
    "    return similar_words\n",
    "\n",
    "similar_words = find_similar_words(tokenKeywordEmbeddings, tokenTextEmbedding)\n",
    "\n",
    "shortenedSimilarWords = {}\n",
    "\n",
    "for keyword, words in similar_words.items():\n",
    "    #print(f\"Words similar to '{keyword}':\")\n",
    "    for word, similarity in words[:50]:\n",
    "        #print(f\" {word}: {similarity}\")\n",
    "        shortenedSimilarWords[keyword] = words[:12]\n",
    "\n",
    "print(shortenedSimilarWords)\n",
    "\n",
    "jsonStorage = \"SimilarWords.json\"\n",
    "with open(jsonStorage, \"w\") as file:\n",
    "    json.dump(shortenedSimilarWords, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
