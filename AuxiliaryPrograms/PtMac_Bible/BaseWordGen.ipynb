{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Arial; font-size: 14pt;\"><b>Finding Top Similar Words as Base Words</b></span><br>\n",
    "Author: Lucas Ma<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lucasma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/Users/lucasma/miniforge3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import string, heapq, numpy as np\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "import os, json\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Make sure to download the punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "model_name = \"emanjavacas/MacBERTh\"\n",
    "# model_name = \"fine-tuned-MacBERTh\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 142\u001b[0m\n\u001b[1;32m    139\u001b[0m target_words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoney\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchrist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdarkness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclothes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaked\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Find and print top 10 similar words\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m top_similar_words \u001b[38;5;241m=\u001b[39m \u001b[43mfind_top_similar_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, words \u001b[38;5;129;01min\u001b[39;00m top_similar_words\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 50 words most similar to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 114\u001b[0m, in \u001b[0;36mfind_top_similar_words\u001b[0;34m(target_words, sentences, tokenizer, model, top_n)\u001b[0m\n\u001b[1;32m    111\u001b[0m similarities \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token, embedding \u001b[38;5;129;01min\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 114\u001b[0m     similarities[token] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkeyWord\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m ((np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(embedding)) \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(key_embeddings[keyWord])))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Use a heap to find the top N similar words\u001b[39;00m\n\u001b[1;32m    117\u001b[0m top_entries \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'dict'"
     ]
    }
   ],
   "source": [
    "def read_sentence_document(document_path):\n",
    "    encodings = ['utf-8', 'latin-1', 'cp1252']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(document_path, 'r', encoding=encoding) as f:\n",
    "                text = f.read()\n",
    "            break  # Exit the loop if no exception was raised\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    else:\n",
    "        raise UnicodeDecodeError(\"Failed to read the file with any of the tried encodings.\")\n",
    "        \n",
    "    text = text.lower()\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Further split the sentences if there are more than 450 words\n",
    "    split_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        while len(words) > 400:\n",
    "            split_sentences.append(' '.join(words[:400]))\n",
    "            words = words[400:]\n",
    "        split_sentences.append(' '.join(words))\n",
    "\n",
    "    # Strip whitespace and filter out empty sentences\n",
    "    final_sentences = [s.strip() for s in split_sentences if s.strip()]\n",
    "    \n",
    "    return final_sentences\n",
    "\n",
    "\n",
    "# Function to get word embeddings\n",
    "def get_word_embedding(chunks, tokenizer, model):\n",
    "    word_times = {}\n",
    "    word_embeddings = {}\n",
    "    for chunk in chunks:\n",
    "\n",
    "        inputs = tokenizer(chunk, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        for i, word in enumerate(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])):\n",
    "            if word not in word_times:\n",
    "                word_times[word] = 1\n",
    "            else:\n",
    "                word_times[word]+=1\n",
    "            if word not in word_embeddings:\n",
    "                word_embeddings[word] = embeddings[0, i, :].numpy()\n",
    "            else:\n",
    "                word_embeddings[word] = (word_embeddings[word] * (word_times[word]-1) + embeddings[0, i, :].numpy()) / word_times[word]\n",
    "    return word_embeddings\n",
    "\n",
    "def get_single_embedding(word, tokenizer, model):\n",
    "    word_embeddings = {}\n",
    "    chunk = word\n",
    "    inputs = tokenizer(chunk, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    for i, word in enumerate(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])):\n",
    "        if word not in word_embeddings:\n",
    "            word_embeddings[word] = embeddings[0, i, :].numpy()\n",
    "    return word_embeddings\n",
    "\n",
    "# Find the substitute words for the original keyword, by iterating over the standard word list\n",
    "def substitute_word(word):\n",
    "    json_dir = \"/Users/lucasma/Documents/The States/ECBC/Code/ECBCData2024/standardizedwords.json\"\n",
    "    with open (json_dir, \"r\") as f:\n",
    "        standardWord = json.load(f)\n",
    "    ret = []\n",
    "    for term, equals in standardWord.items():\n",
    "        for spell in equals:\n",
    "            if spell==word:\n",
    "                ret.append(term)\n",
    "    return ret\n",
    "\n",
    "def clean_embedding(embeddings):\n",
    "    clean = {}\n",
    "    for token, embedding in embeddings.items():\n",
    "        if (token in tokenizer.all_special_tokens) or (token.lower() in stop_words) or (token in string.punctuation) or (token.startswith('##')) or (token==\"â€¢\"):\n",
    "            clean[token] = embedding\n",
    "    return clean\n",
    "\n",
    "# Main function\n",
    "def find_top_similar_words(target_words, sentences, tokenizer, model, top_n):\n",
    "    \n",
    "    embeddings = clean_embedding(get_word_embedding(sentences, tokenizer, model))\n",
    "\n",
    "    key_sim_word = {}\n",
    "\n",
    "    for keyWord in target_words:\n",
    "        key_embeddings = {}\n",
    "        presence = \"\"\n",
    "        wordFound = False\n",
    "\n",
    "        if keyWord in embeddings:\n",
    "            presence = keyWord\n",
    "            wordFound = True\n",
    "        else:\n",
    "            for equal in substitute_word(keyWord):\n",
    "                wordFound = wordFound or equal in embeddings\n",
    "                if wordFound:\n",
    "                    presence = equal\n",
    "                    break\n",
    "        \n",
    "        if not wordFound:\n",
    "            key_embeddings[keyWord] = get_single_embedding(keyWord, tokenizer, model)\n",
    "        else:\n",
    "            key_embeddings[keyWord] = embeddings[presence]\n",
    "\n",
    "        similarities = {}\n",
    "\n",
    "        for token, embedding in embeddings.items():\n",
    "            similarities[token] = np.dot(embedding, key_embeddings[keyWord]) / ((np.linalg.norm(embedding)) * (np.linalg.norm(key_embeddings[keyWord])))\n",
    "    \n",
    "        # Use a heap to find the top N similar words\n",
    "        top_entries = []\n",
    "        #seen_words = set()\n",
    "        for token, similarity in similarities:\n",
    "                heapq.heappush(top_entries, (token, similarity))\n",
    "\n",
    "                if len(top_entries) > top_n:\n",
    "                    removed_token, removed_similarity = heapq.heappop(top_entries)\n",
    "\n",
    "        top_entries.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "        key_sim_word[keyWord] = top_entries\n",
    "\n",
    "    return key_sim_word\n",
    "\n",
    "\n",
    "# Path to the .txt file\n",
    "# file_path = '/Users/lucasma/Documents/The\\ States/ECBC/Code/ECBCData2024/data/VirginiaTotal.txt'\n",
    "file_path = \"data/A10010_short.txt\"\n",
    "\n",
    "sentences = read_sentence_document(file_path)\n",
    "\n",
    "# Target word to find similarities with\n",
    "target_words = [\"money\", \"christ\", \"light\", \"darkness\", \"clothes\", \"naked\"]\n",
    "\n",
    "# Find and print top 10 similar words\n",
    "top_similar_words = find_top_similar_words(target_words, sentences, tokenizer, model, 50)\n",
    "\n",
    "for word, words in top_similar_words.items():\n",
    "    print(f\"Top 50 words most similar to '{word}':\")\n",
    "    for relevantWord, sim in words.items():\n",
    "        print(f\"{relevantWord}: {sim}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
