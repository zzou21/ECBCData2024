{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practicing using BERT through movie review sentiment classification.\n",
    "\n",
    "Author: Jerry Zou\n",
    "\n",
    "Because MacBERTh is built on BERT, there only needs minimum change to the code in this file when we transition to MacBERTh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importing libraries...\")\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import pyarrow as pa\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"Libraries uploaded.\")\n",
    "dataFrame = pd.read_csv(\"IMDBDataset.csv\")\n",
    "dataFrame.head()\n",
    "print(\"Importing tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"Tokenizer imported.\")\n",
    "\n",
    "def processData(row):\n",
    "    text = row[\"review\"]\n",
    "    text = str(text)\n",
    "    text = \" \".join(text.split())\n",
    "    encoder = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "    label = 0\n",
    "    if row[\"sentiment\"] == \"positive\":\n",
    "        label +=1\n",
    "    \n",
    "    encoder[\"label\"] = label\n",
    "    encoder[\"text\"] = text\n",
    "    return encoder\n",
    "\n",
    "print(\"Processing...\")\n",
    "print(processData({\n",
    "    \"review\": \"this is a sample review of a movie.\",\n",
    "    \"sentiment\": \"positive\"\n",
    "}))\n",
    "\n",
    "processedData = []\n",
    "for i in range(len(dataFrame[:100])):\n",
    "    processedData.append(processData(dataFrame.iloc[i])) \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "new_df = pd.DataFrame(processedData)\n",
    "\n",
    "train_df, valid_df = train_test_split(\n",
    "    new_df,\n",
    "    test_size=0.2,\n",
    "    random_state=2022\n",
    ")\n",
    "\n",
    "train_hg = Dataset(pa.Table.from_pandas(train_df))\n",
    "valid_hg = Dataset(pa.Table.from_pandas(valid_df))\n",
    "print(type(train_hg))\n",
    "print(type(valid_hg))\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"./result\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_hg,\n",
    "    eval_dataset=valid_hg,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(type(train_hg))\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "model.save_pretrained('./modelTrained/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 15:58:33.851891: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries uploaded.\n",
      "Importing tokenizer...\n",
      "Tokenizer imported.\n",
      "Processing...\n",
      "{'input_ids': [101, 2023, 2003, 1037, 7099, 3319, 1997, 1037, 3185, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'text': 'this is a sample review of a movie.'}\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "#load the model\n",
    "newModel = AutoModelForSequenceClassification.from_pretrained(\"modelTrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m newTokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "newTokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as mp\n",
    "def getPrediction(text):\n",
    "    encoding = newTokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "    encoding = {k: v.to(textclassificationpractice.trainer.model.device) for k, v in encoding.items()}\n",
    "    outputs = newModel(**encoding)\n",
    "    logits = outputs.logits\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(logits.squeeze().cpu())\n",
    "    probs = probs.detach().numpy()\n",
    "    label = mp.argmax(probs, axis=-1)\n",
    "\n",
    "    if label == 1:\n",
    "        return{\n",
    "            \"sentiment\": \"Positive\",\n",
    "            \"probability\": probs[1]\n",
    "        }\n",
    "    else:\n",
    "        return{\n",
    "            \"sentiment\": \"Negative\",\n",
    "            \"probability\": probs[0]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getPrediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgetPrediction\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love today\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms weather.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getPrediction' is not defined"
     ]
    }
   ],
   "source": [
    "getPrediction(\"I love today's weather.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
