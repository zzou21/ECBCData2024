{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook file exists so that we have a cleaner debugging environment. The code below, after it's settled, is to be copied into the file Bias_Identification.py for submission to DCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lucasma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /Users/lucasma/Documents/The States/ECBC/Code/ECBCData2024/Bias Identification/../data/fine-tuned-MacBERTh and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum sentence length is: 2432 characters\n",
      "Warning: Word 'conversion' not found in embeddings.\n",
      "Warning: Word 'Jesus' not found in embeddings.\n",
      "Warning: Word 'trinity' not found in embeddings.\n",
      "Warning: Word 'divine' not found in embeddings.\n",
      "Warning: Word 'fervor' not found in embeddings.\n",
      "Warning: Word 'piety' not found in embeddings.\n",
      "Warning: Word 'blessed' not found in embeddings.\n",
      "Warning: Word 'sanctified' not found in embeddings.\n",
      "Warning: Word 'crude' not found in embeddings.\n",
      "Warning: Word 'greek' not found in embeddings.\n",
      "Warning: Word 'Aristotle' not found in embeddings.\n",
      "Warning: Word 'pure' not found in embeddings.\n",
      "Warning: Word 'Eden' not found in embeddings.\n",
      "Warning: Word 'sermon' not found in embeddings.\n",
      "Warning: Word 'education' not found in embeddings.\n",
      "Warning: Word 'currency' not found in embeddings.\n",
      "Warning: Word 'finance' not found in embeddings.\n",
      "Warning: Word 'Marco Polo' not found in embeddings.\n",
      "Warning: Word 'precious' not found in embeddings.\n",
      "Warning: Word 'gem' not found in embeddings.\n",
      "Warning: Word 'affluence' not found in embeddings.\n",
      "Warning: Word 'labor' not found in embeddings.\n",
      "Warning: Word 'luxury' not found in embeddings.\n",
      "Warning: Word 'revenue' not found in embeddings.\n",
      "Warning: Word 'Asia' not found in embeddings.\n",
      "Warning: Word 'spice' not found in embeddings.\n",
      "Warning: Word 'silkworm' not found in embeddings.\n",
      "Warning: Word 'exquisite' not found in embeddings.\n",
      "Warning: Word 'expenseful' not found in embeddings.\n",
      "Warning: Word 'expenditure' not found in embeddings.\n",
      "Warning: Word 'piracy' not found in embeddings.\n",
      "Warning: Word 'privateering' not found in embeddings.\n",
      "Warning: Word 'greed' not found in embeddings.\n",
      "Warning: Word 'charitable' not found in embeddings.\n",
      "Warning: Word 'afterlife' not found in embeddings.\n",
      "Warning: Word 'freedom' not found in embeddings.\n",
      "Warning: Word 'ideal' not found in embeddings.\n",
      "Warning: Word 'spacious' not found in embeddings.\n",
      "Warning: Word 'excess' not found in embeddings.\n",
      "Warning: Word 'fertile' not found in embeddings.\n",
      "Warning: Word 'uneducated' not found in embeddings.\n",
      "Warning: Word 'opportunity' not found in embeddings.\n",
      "Warning: Word 'protestant' not found in embeddings.\n",
      "Warning: Word 'harvest' not found in embeddings.\n",
      "Warning: Word 'godly' not found in embeddings.\n",
      "Warning: Word 'god-given' not found in embeddings.\n",
      "Warning: Word 'infidel' not found in embeddings.\n",
      "Warning: Word 'grim' not found in embeddings.\n",
      "Warning: Word 'destitute' not found in embeddings.\n",
      "Warning: Word 'loss' not found in embeddings.\n",
      "Warning: Word 'forsaken' not found in embeddings.\n",
      "Warning: Word 'impossible' not found in embeddings.\n",
      "Warning: Word 'starvation' not found in embeddings.\n",
      "Warning: Word 'barbarous' not found in embeddings.\n",
      "Warning: Word 'barren' not found in embeddings.\n",
      "Warning: Word 'cursed' not found in embeddings.\n",
      "Warning: Word 'sacrilegious' not found in embeddings.\n",
      "Warning: Word 'pagan' not found in embeddings.\n",
      "Warning: Word 'hunger' not found in embeddings.\n",
      "Warning: Word 'suffering' not found in embeddings.\n",
      "Warning: Word 'heathing' not found in embeddings.\n",
      "Warning: Word 'uneducated' not found in embeddings.\n",
      "Warning: Word 'lewd' not found in embeddings.\n",
      "Warning: Word 'scary' not found in embeddings.\n",
      "Warning: Word 'terrifying' not found in embeddings.\n",
      "Projection of 'profit' onto Faith-Money axis: -5.938217639923096\n",
      "Projection of 'profit' onto Attraction-Repulsion axis: 2.15385365486145\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Make sure to download the punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load categories from JSON file\n",
    "def load_categories(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        categories = json.load(f)\n",
    "    return categories\n",
    "\n",
    "# Function to read a document and break it into sentences\n",
    "def read_sentence_document(document_path):\n",
    "    with open(document_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Tokenize the text into sentences using nltk\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Further split the sentences if there are more than 450 words\n",
    "    split_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        while len(words) > 450:\n",
    "            split_sentences.append(' '.join(words[:450]))\n",
    "            words = words[450:]\n",
    "        split_sentences.append(' '.join(words))\n",
    "\n",
    "    # Strip whitespace and filter out empty sentences\n",
    "    final_sentences = [s.strip() for s in split_sentences if s.strip()]\n",
    "    \n",
    "    return final_sentences\n",
    "\n",
    "# Function to get word embeddings\n",
    "def get_word_embedding(chunks, tokenizer, model):\n",
    "    word_embeddings = {}\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        for i, word in enumerate(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])):\n",
    "            if word not in word_embeddings:\n",
    "                word_embeddings[word] = embeddings[0, i, :].numpy()\n",
    "    return word_embeddings\n",
    "\n",
    "# Compute average vector for each category\n",
    "def compute_category_embeddings(categories, embeddings):\n",
    "    categories_embeddings = {}\n",
    "    for category, words in categories.items():\n",
    "        category_embeddings = []\n",
    "        for word in words:\n",
    "            if word in embeddings:\n",
    "                embedding = embeddings[word]\n",
    "                category_embeddings.append(embedding)\n",
    "            else:\n",
    "                print(f\"Warning: Word '{word}' not found in embeddings.\")\n",
    "        if category_embeddings:\n",
    "            categories_embeddings[category] = np.mean(category_embeddings, axis=0)\n",
    "        else:\n",
    "            print(f\"Warning: No embeddings found for category '{category}'.\")\n",
    "    return categories_embeddings\n",
    "\n",
    "# Construct bias axes\n",
    "def construct_bias_axes(category_embeddings):\n",
    "    faith_bias_axis = category_embeddings[\"Faith\"] - category_embeddings[\"Money\"]\n",
    "    desire_bias_axis = category_embeddings[\"Attraction\"] - category_embeddings[\"Repulsion\"]\n",
    "    return faith_bias_axis, desire_bias_axis\n",
    "\n",
    "# Function to project a word onto bias axes\n",
    "def project_onto_bias_axis(word, embeddings, bias_axis):\n",
    "    if word in embeddings:\n",
    "        embedding = embeddings[word]\n",
    "        projection = np.dot(embedding, bias_axis.T) / np.linalg.norm(bias_axis)\n",
    "        return projection\n",
    "    else:\n",
    "        print(f\"Error: Word '{word}' not found in embeddings.\")\n",
    "        return None\n",
    "\n",
    "# Main function\n",
    "def main(categories_json, document_path, model_name, keyword):\n",
    "    # Load categories\n",
    "    categories = load_categories(categories_json)\n",
    "    \n",
    "    # Load the MacBERTh model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Read and tokenize the document\n",
    "    sentences = read_sentence_document(document_path)\n",
    "\n",
    "    print(f\"The maximum sentence length is: {len(max(sentences, key=len))} characters\")\n",
    "\n",
    "    embeddings = get_word_embedding(sentences, tokenizer, model)\n",
    "    \n",
    "    # Compute category embeddings\n",
    "    category_embeddings = compute_category_embeddings(categories, embeddings)\n",
    "    \n",
    "    # Construct bias axes\n",
    "    faith_bias_axis, desire_bias_axis = construct_bias_axes(category_embeddings)\n",
    "    \n",
    "    # Example: Project words from the document onto bias axes\n",
    "    projection_faith = project_onto_bias_axis(keyword, embeddings, faith_bias_axis)\n",
    "    projection_desire = project_onto_bias_axis(keyword, embeddings, desire_bias_axis)\n",
    "\n",
    "    if projection_faith is not None:\n",
    "        print(f\"Projection of '{keyword}' onto Faith-Money axis: {projection_faith}\")\n",
    "    if projection_desire is not None:\n",
    "        print(f\"Projection of '{keyword}' onto Attraction-Repulsion axis: {projection_desire}\")\n",
    "\n",
    "# Now this is the main; feel free to change the following directory where fit\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "keyword = \"profit\"\n",
    "categories_json = os.path.join(base_dir, '..', 'data/categorized_words.json')\n",
    "document_path = os.path.join(base_dir, '..', 'data/copland_spellclean.txt')\n",
    "model_name = os.path.join(base_dir, '..', 'data/fine-tuned-MacBERTh')\n",
    "# model_name = model_name = os.path.join(base_dir, '..', 'data/emanjavacas/MacBERTh')\n",
    "main(categories_json, document_path, model_name, keyword)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
